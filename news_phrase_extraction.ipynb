{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install azure-ai-textanalytics==5.2.0b1\r\n",
    "!pip install bs4\r\n",
    "!pip install requests"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# authenticate azure text analytics client\r\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\r\n",
    "from azure.core.credentials import AzureKeyCredential\r\n",
    "\r\n",
    "key = \"677e7a68d2f94932a97572bfd9505ea3\"\r\n",
    "endpoint = \"https://covid-corruption-nlp2.cognitiveservices.azure.com/\"\r\n",
    "\r\n",
    "def authenticate_client():\r\n",
    "    ta_credential = AzureKeyCredential(key)\r\n",
    "    text_analytics_client = TextAnalyticsClient(\r\n",
    "            endpoint=endpoint, \r\n",
    "            credential=ta_credential)\r\n",
    "    return text_analytics_client\r\n",
    "\r\n",
    "client = authenticate_client()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# perform key phrase extraction\r\n",
    "def key_phrase_extraction(client, documents=\"\"):\r\n",
    "    try:\r\n",
    "        responses = client.extract_key_phrases(documents = documents)\r\n",
    "        \r\n",
    "        key_phrases = []\r\n",
    "        for response in responses:\r\n",
    "            if response.key_phrases:\r\n",
    "                key_phrases += response.key_phrases\r\n",
    "        \r\n",
    "        return key_phrases\r\n",
    "\r\n",
    "    except Exception as err:\r\n",
    "        print(\"Encountered exception. {}\".format(err))\r\n",
    "\r\n",
    "\r\n",
    "news_source = \"sky\"\r\n",
    "f_in = open('datasets/parsed-news/{}.txt'.format(news_source), 'r')\r\n",
    "documents = [line.rstrip() for line in f_in.readlines()]\r\n",
    "num_docs = len(documents)\r\n",
    "\r\n",
    "key_phrases = []\r\n",
    "for i in range(0, num_docs, 10):\r\n",
    "    print(i)\r\n",
    "    key_phrase = key_phrase_extraction(\r\n",
    "                    client, \r\n",
    "                    documents=documents[i: min(num_docs, i+10)]\r\n",
    "    )\r\n",
    "\r\n",
    "    if key_phrase:\r\n",
    "        key_phrases += key_phrase\r\n",
    "\r\n",
    "print(key_phrases)\r\n",
    "f_out = open('datasets/parsed-news/{}_phrases.txt'.format(news_source), 'w+')\r\n",
    "for phrase in key_phrases:\r\n",
    "    phrase = '-'.join(phrase.split())\r\n",
    "    f_out.write(phrase + '\\n')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# opinion mining on trustpilot reviews\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "import requests\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "grouped_details = pd.read_csv('datasets/grouped_details.csv')\r\n",
    "link_cols = [x for x in grouped_details.columns if 'company_link' in x]\r\n",
    "\r\n",
    "def get_reviews(x):\r\n",
    "    results = []\r\n",
    "    for cell in x:\r\n",
    "        if pd.isna(cell) or not cell:\r\n",
    "            results.append('')\r\n",
    "        else:\r\n",
    "            page = requests.get(\"https://uk.trustpilot.com/review/\"+cell)\r\n",
    "            print(page)\r\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\r\n",
    "            reviewcontent = soup.find_all('p', {'class', 'review-content__text'})\r\n",
    "            \r\n",
    "            results.append([review.get_text() for review in reviewcontent if review])\r\n",
    "    return results\r\n",
    "\r\n",
    "link_details = pd.DataFrame(grouped_details[link_cols])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# tr_reviews = link_details.apply(get_reviews)\r\n",
    "# tr_reviews.to_csv('reviews.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def opinion_mining(client,documents):\r\n",
    "\r\n",
    "    result = client.analyze_sentiment(documents, show_opinion_mining=True)\r\n",
    "    doc_result = [doc for doc in result if not doc.is_error]\r\n",
    "\r\n",
    "    negative_keywords=[]\r\n",
    "    for document in doc_result:\r\n",
    "        for sentence in document.sentences:\r\n",
    "            for mined_opinion in sentence.mined_opinions:\r\n",
    "                if mined_opinion.target.sentiment == 'negative':\r\n",
    "                    negative_keywords.append(mined_opinion.assessments[0].text)\r\n",
    "                else:\r\n",
    "                    negative_keywords.append('')\r\n",
    "    return negative_keywords \r\n",
    "\r\n",
    "tr_reviews =  pd.read_csv('reviews.csv')\r\n",
    "keywords = []\r\n",
    "for review in tr_reviews['company_link_0']:\r\n",
    "    #print(type(review))\r\n",
    "    if type(review) == str: \r\n",
    "        review = eval(review)\r\n",
    "        num_docs = len(review)\r\n",
    "        for i in range(0, num_docs, 10):\r\n",
    "            keywords.append(opinion_mining(client,documents=review[i: min(num_docs, i+10)]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "keywords = pd.DataFrame(keywords)\r\n",
    "keywords['company_name'] = grouped_details['company_name']\r\n",
    "keywords.to_csv('datasets/tp_keywords.csv')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit (windows store)"
  },
  "interpreter": {
   "hash": "1e278ca4e9a2ae999502812da3aa9042213e3ce53c46b5f0452fc22ced78fb5d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}