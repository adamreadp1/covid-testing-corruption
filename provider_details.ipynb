{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Copy of Company Details (Google).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.7  ('covid_corruption_venv': venv)"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "5f8aec9e558577a87b5b61cddb74ed49406d2acbe45acb8a6a200a35f976c5eb"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# download & import libraries\r\n",
        "!pip install --upgrade pip\r\n",
        "!pip install bs4\r\n",
        "!pip install pandas\r\n",
        "!pip install requests\r\n",
        "!pip install numpy\r\n",
        "!pip install urllib3\r\n",
        "!pip install phonenumbers\r\n",
        "!pip install py3-validate-email\r\n",
        "!pip install fuzzywuzzy\r\n",
        "\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import pandas as pd\r\n",
        "import requests\r\n",
        "import urllib\r\n",
        "from urllib.parse import urlparse\r\n",
        "import numpy as np\r\n",
        "import phonenumbers\r\n",
        "from validate_email import validate_email\r\n",
        "from fuzzywuzzy.fuzz import partial_ratio\r\n",
        "import csv\r\n",
        "import json\r\n",
        "import os"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# timestamps of gvmt certified test providers\r\n",
        "urls = [ \r\n",
        "    'https://web.archive.org/web/20210519121145/https://www.find-travel-test-provider.service.gov.uk/test-type/amber',\r\n",
        "    'https://web.archive.org/web/20210520140910/https://www.find-travel-test-provider.service.gov.uk/test-type/amber',\r\n",
        "    'https://web.archive.org/web/20210526072031/https://www.find-travel-test-provider.service.gov.uk/test-type/amber',\r\n",
        "    'https://web.archive.org/web/20210529103556/https://www.find-travel-test-provider.service.gov.uk/test-type/amber',\r\n",
        "    'https://web.archive.org/web/20210531150239/https://www.find-travel-test-provider.service.gov.uk/test-type/amber',\r\n",
        "    'https://web.archive.org/web/20210604100539/https://www.find-travel-test-provider.service.gov.uk/test-type/amber',\r\n",
        "    'https://web.archive.org/web/20210618121419/https://www.find-travel-test-provider.service.gov.uk/test-type/amber',\r\n",
        "    'https://web.archive.org/web/20210626230824/https://www.find-travel-test-provider.service.gov.uk/test-type/amber',\r\n",
        "    'https://web.archive.org/web/20210629104343/https://www.find-travel-test-provider.service.gov.uk/test-type/amber',\r\n",
        "    'https://web.archive.org/web/20210701142411/https://www.find-travel-test-provider.service.gov.uk/test-type/amber',\r\n",
        "    'https://web.archive.org/web/20210708123655/https://www.find-travel-test-provider.service.gov.uk/test-type/amber',\r\n",
        "    'https://web.archive.org/web/20210713165043/https://www.find-travel-test-provider.service.gov.uk/test-type/amber',\r\n",
        "    'https://web.archive.org/web/20210729180818/https://www.find-travel-test-provider.service.gov.uk/test-type/amber',\r\n",
        "    'https://web.archive.org/web/20210808132243/https://www.find-travel-test-provider.service.gov.uk/test-type/amber',\r\n",
        "    'https://web.archive.org/web/20210811073050/https://www.find-travel-test-provider.service.gov.uk/test-type/amber',\r\n",
        "    'https://web.archive.org/web/20210812113842/https://www.find-travel-test-provider.service.gov.uk/test-type/amber',\r\n",
        "    'https://web.archive.org/web/20210817114915/https://www.find-travel-test-provider.service.gov.uk/test-type/amber',\r\n",
        "    'https://web.archive.org/web/20210818174329/https://www.find-travel-test-provider.service.gov.uk/test-type/amber',\r\n",
        "    'https://web.archive.org/web/20210823032256/https://www.find-travel-test-provider.service.gov.uk/test-type/amber',\r\n",
        "    'https://web.archive.org/web/20210824065330/https://www.find-travel-test-provider.service.gov.uk/test-type/amber',\r\n",
        "    'https://www.find-travel-test-provider.service.gov.uk/test-type/amber'\r\n",
        "]\r\n",
        "\r\n",
        "provider_details = pd.DataFrame(columns=['company_name','company_link', 'company_number', 'company_email', 'price'])\r\n",
        "\r\n",
        "for url in urls:\r\n",
        "    print(url)\r\n",
        "    # scrape provider table for each time stamp\r\n",
        "    page = requests.get(url)\r\n",
        "    soup = BeautifulSoup(page.text, 'html.parser')\r\n",
        "    providers_table = soup.find('table', {'class', 'govuk-table'}).find('tbody')\r\n",
        "\r\n",
        "    url_provider_details = pd.DataFrame(columns=['company_name', 'company_link', 'company_number', 'company_email'])\r\n",
        "    for row in providers_table.find_all('tr'):\r\n",
        "        # test provider saved in cell with id 'provider'\r\n",
        "        provider = row.find(id='provider').find('a')\r\n",
        "\r\n",
        "        # scrape name and link of provider\r\n",
        "        name = provider.get_text().rstrip().lower()\r\n",
        "        link = provider['href']\r\n",
        "    \r\n",
        "        # test provider number and email saved in only cell(s) with no id\r\n",
        "        number_email = row.find_all('td', id=None)\r\n",
        "        \r\n",
        "        # remove web archive prefix from wayback machine\r\n",
        "        link = link[43:] if 'web.archive.org' in link else link\r\n",
        "\r\n",
        "        # old format stores number and email in separate cells\r\n",
        "        # new format stores number and email in same cell\r\n",
        "        if len(number_email) == 1:\r\n",
        "            number_email = number_email[0].find_all('a')\r\n",
        "        number = str(number_email[0].get_text())\r\n",
        "        email = number_email[1].get_text()\r\n",
        "\r\n",
        "        # test provider price saved in cell with id 'priceAmber'\r\n",
        "        price = float(row.find(id='priceAmber').get_text().replace('Â£', ''))\r\n",
        "        \r\n",
        "        # clean scraped data\r\n",
        "        if number and len(number) >= 10:\r\n",
        "            number = ' '.join(number.rstrip().split())\r\n",
        "            number = phonenumbers.format_number(phonenumbers.parse(number, 'GB'), phonenumbers.PhoneNumberFormat.INTERNATIONAL)\r\n",
        "        else:\r\n",
        "            number = np.nan \r\n",
        "        if email:\r\n",
        "            email = str(email).rstrip().replace('\\n', '').lower()\r\n",
        "        if link:\r\n",
        "            link = urlparse(link.lower()).netloc\r\n",
        "\r\n",
        "        # add provider details to df\r\n",
        "        url_provider_details = url_provider_details.append({\r\n",
        "            'company_name': name,\r\n",
        "            'company_link': link,\r\n",
        "            'company_number': number,\r\n",
        "            'company_email': email,\r\n",
        "            'price': price\r\n",
        "        }, ignore_index=True)\r\n",
        "\r\n",
        "    # merge provider details across urls\r\n",
        "    provider_details = pd.merge(\r\n",
        "        provider_details, \r\n",
        "        url_provider_details, \r\n",
        "        how=\"outer\", \r\n",
        "        on=['company_name','company_link','company_number','company_email', 'price']\r\n",
        "    )\r\n",
        "\r\n",
        "provider_details.to_csv('./datasets/aggregated-details/provider_details.csv', index=False)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1ff0gsQpq9r",
        "outputId": "922023be-7a57-4a15-8bf1-1d7426b7feb7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "source": [
        "# group all numbers, emails, and links for each company together\r\n",
        "grouped_details = pd.DataFrame(columns=['company_name'])\r\n",
        "provider_details = pd.read_csv('./datasets/aggregated-details/provider_details.csv')\r\n",
        "\r\n",
        "for category in ['company_number', 'company_email', 'company_link', 'price']:\r\n",
        "    # remove duplicate scraped providers\r\n",
        "    category_details = provider_details[['company_name', category]].drop_duplicates()\r\n",
        "\r\n",
        "    # group providers together by name (i.e. group all given prices/emails/number for a provider)\r\n",
        "    df = (category_details.set_index(['company_name', category_details.groupby('company_name').cumcount()])[category]\r\n",
        "            .unstack(fill_value='')\r\n",
        "            .add_prefix(category+'_')\r\n",
        "            .reset_index())\r\n",
        "    grouped_details = pd.merge(grouped_details, df, how='outer',on=['company_name'])\r\n",
        "\r\n",
        "grouped_details.to_csv('./datasets/aggregated-details/grouped_details.csv', index=False)"
      ],
      "outputs": [],
      "metadata": {
        "id": "RqV0oOlhqP3x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# produce risk score based on price data\r\n",
        "grouped_price = pd.read_csv('./datasets/aggregated-details/grouped_details.csv')\r\n",
        "price_cols = [x for x in grouped_price if 'price' in x]\r\n",
        "\r\n",
        "price_details = pd.DataFrame(columns=['num_matches', 'num_jumps', 'max_change'])\r\n",
        "for company in grouped_price['company_name']:\r\n",
        "    print(company)\r\n",
        "    # match identified providers with similar names \r\n",
        "    fuzzy_matches = [ind for ind in grouped_price.index if partial_ratio(company, grouped_price['company_name'][ind])>=90]\r\n",
        "\r\n",
        "    matched_rows = grouped_price.loc[fuzzy_matches][price_cols]\r\n",
        "    mean_prices = matched_rows.mean(axis=0).dropna() # average across similar providers\r\n",
        "\r\n",
        "    num_jumps = max_change = 0\r\n",
        "    prev_price = mean_prices[0]\r\n",
        "    for price in mean_prices[1:]:\r\n",
        "        # track max price change and total price changes\r\n",
        "        if prev_price != price:\r\n",
        "            num_jumps += 1\r\n",
        "            change = abs((price - prev_price)/prev_price)\r\n",
        "\r\n",
        "            if change > max_change:\r\n",
        "                max_change = change\r\n",
        "        \r\n",
        "        prev_price = price\r\n",
        "\r\n",
        "    # add price signals to df\r\n",
        "    price_details = price_details.append({\r\n",
        "        'num_matches': len(fuzzy_matches), \r\n",
        "        'num_jumps': num_jumps,\r\n",
        "        'max_change': max_change*100\r\n",
        "    }, ignore_index=True)\r\n",
        "\r\n",
        "\r\n",
        "price_details['company_name'] = grouped_price['company_name']\r\n",
        "price_details.to_csv('./datasets/indicators/price_details.csv', index=False)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# validate scraped phone numbers\r\n",
        "grouped_details = pd.read_csv('./datasets/aggregated-details/grouped_details.csv')\r\n",
        "phone_cols = [x for x in grouped_details.columns if 'company_number' in x]\r\n",
        "\r\n",
        "def check_number(x):\r\n",
        "    results = []\r\n",
        "    for cell in x:\r\n",
        "        if pd.isna(cell) or len(str(cell)) < 9:\r\n",
        "            results.append('')\r\n",
        "        else:\r\n",
        "            number = phonenumbers.parse(cell,'GB')\r\n",
        "            valid = phonenumbers.is_valid_number(number)\r\n",
        "            results.append(bool(valid))\r\n",
        "    return results\r\n",
        "\r\n",
        "phone_details = grouped_details[phone_cols]\r\n",
        "phone_validated = phone_details.apply(check_number)\r\n",
        "invalid_number = pd.DataFrame({\r\n",
        "    'company_name': grouped_details['company_name'], \r\n",
        "    'phone_invalid': [False]*len(grouped_details['company_name'])\r\n",
        "})\r\n",
        "\r\n",
        "invalid_cols = phone_validated[phone_validated.eq(False).any(axis=1)].index\r\n",
        "empty_cols = phone_validated[phone_validated.eq('').all(axis=1)].index\r\n",
        "\r\n",
        "invalid_number['phone_invalid'].iloc[invalid_cols] = True\r\n",
        "invalid_number['phone_invalid'].iloc[empty_cols] = True\r\n",
        "\r\n",
        "invalid_number.to_csv('./datasets/indicators/phone_validation.csv', index=False)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# validate scraped email addresses\r\n",
        "grouped_details = pd.read_csv('./datasets/aggregated-details/grouped_details.csv')\r\n",
        "email_cols = [x for x in grouped_details.columns if 'company_email' in x]\r\n",
        "\r\n",
        "def check_email(x):\r\n",
        "    results = []\r\n",
        "    for cell in x:\r\n",
        "        print(str(cell).rstrip())\r\n",
        "        if pd.isna(cell) or not cell:\r\n",
        "            results.append('')\r\n",
        "        else:\r\n",
        "            valid = validate_email(email_address=str(cell).rstrip())\r\n",
        "            results.append(bool(valid))\r\n",
        "    return results\r\n",
        "    \r\n",
        "\r\n",
        "email_details = grouped_details[email_cols]\r\n",
        "email_validated = email_details.apply(check_email)\r\n",
        "invalid_email = pd.DataFrame({\r\n",
        "    'company_name': grouped_details['company_name'], \r\n",
        "    'email_invalid': [False]*len(grouped_details['company_name'])\r\n",
        "})\r\n",
        "\r\n",
        "\r\n",
        "invalid_cols = email_validated[email_validated.eq(False).any(axis=1)].index\r\n",
        "empty_cols = email_validated[email_validated.eq('').all(axis=1)].index\r\n",
        "\r\n",
        "invalid_email['email_invalid'].iloc[invalid_cols] = True\r\n",
        "invalid_email['email_invalid'].iloc[empty_cols] = True\r\n",
        "\r\n",
        "invalid_email.to_csv('./datasets/indicators/email_validation.csv', index=False)\r\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# obtaining trustpilot review scores\r\n",
        "grouped_details = pd.read_csv('./datasets/aggregated-details/grouped_details.csv')\r\n",
        "link_cols = [x for x in grouped_details.columns if 'company_link' in x]\r\n",
        "\r\n",
        "def check_trustpilot(x):\r\n",
        "    results = []\r\n",
        "    print(len(x))\r\n",
        "    for cell in x:\r\n",
        "        if pd.isna(cell) or not cell:\r\n",
        "            results.append(np.nan)\r\n",
        "        else:\r\n",
        "            # scrape trust pilot score for each company\r\n",
        "            page = requests.get(\"https://uk.trustpilot.com/review/\"+cell)\r\n",
        "            print(page)\r\n",
        "            soup = BeautifulSoup(page.content, \"html.parser\")\r\n",
        "            score = soup.find('p', {'class', 'header_trustscore'})\r\n",
        "                \r\n",
        "            results.append(float(score.get_text()) if score else np.nan)\r\n",
        "\r\n",
        "    return results\r\n",
        "\r\n",
        "\r\n",
        "link_details = grouped_details[link_cols]\r\n",
        "trustpilot_scores = link_details.apply(check_trustpilot)\r\n",
        "trustpilot_scores[trustpilot_scores.eq(0)] = np.nan \r\n",
        "\r\n",
        "trustpilot_scores['trustpilot_score'] = trustpilot_scores.mean(axis=1)\r\n",
        "\r\n",
        "# categorise scores\r\n",
        "trustpilot_scores['score_category'] = ['']*len(grouped_details)\r\n",
        "trustpilot_scores['score_category'].iloc[trustpilot_scores[trustpilot_scores['trustpilot_score'].isna()].index] = \"No Score\"\r\n",
        "trustpilot_scores['score_category'].iloc[trustpilot_scores[trustpilot_scores['trustpilot_score'].between(0, 1, inclusive=\"right\")].index] = \"0-1\"\r\n",
        "trustpilot_scores['score_category'].iloc[trustpilot_scores[trustpilot_scores['trustpilot_score'].between(1, 2, inclusive=\"right\")].index] = \"1-2\"\r\n",
        "trustpilot_scores['score_category'].iloc[trustpilot_scores[trustpilot_scores['trustpilot_score'].between(2, 3, inclusive=\"right\")].index] = \"2-3\"\r\n",
        "trustpilot_scores['score_category'].iloc[trustpilot_scores[trustpilot_scores['trustpilot_score'].between(3, 4, inclusive=\"right\")].index] = \"3-4\"\r\n",
        "trustpilot_scores['score_category'].iloc[trustpilot_scores[trustpilot_scores['trustpilot_score'].between(4, 5, inclusive=\"right\")].index] = \"4-5\"\r\n",
        "\r\n",
        "trustpilot_scores['is_reviewed'] = [True]*len(grouped_details)\r\n",
        "trustpilot_scores['is_reviewed'].iloc[trustpilot_scores[trustpilot_scores['trustpilot_score'].isna()].index] = False\r\n",
        "\r\n",
        "trustpilot_scores['company_name'] = grouped_details['company_name']\r\n",
        "trustpilot_scores[['trustpilot_score', 'score_category', 'is_reviewed', 'company_name']].to_csv('./datasets/indicators/trustpilot_scores.csv', index=False)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# match registered CH companies\r\n",
        "api_key = '3511f785-bc60-40b3-b697-36bedb63c848'\r\n",
        "\r\n",
        "grouped_details = pd.read_csv('./datasets/aggregated-details/grouped_details.csv')\r\n",
        "matched_companies = pd.DataFrame(columns=['company_name', 'registered_name'])\r\n",
        "matched_companies['company_name'] = grouped_details['company_name']\r\n",
        "\r\n",
        "matched = []\r\n",
        "for company in grouped_details['company_name']:\r\n",
        "    match = ''\r\n",
        "\r\n",
        "    company = company.lower()\r\n",
        "    response = requests.get(\r\n",
        "        'https://api.company-information.service.gov.uk/search?items_per_page=1&q='+'+'.join(company.split()), \r\n",
        "        auth=(api_key, '')\r\n",
        "    )\r\n",
        "    print(response)\r\n",
        "\r\n",
        "    # if top company found on CH is similar to company name consider it registsered\r\n",
        "    if response:\r\n",
        "        response_json = response.json()\r\n",
        "        if response_json['total_results'] > 1:\r\n",
        "            company_found = response.json()['items'][0]['title'].lower()\r\n",
        "\r\n",
        "            if partial_ratio(company_found, company) >= 80:\r\n",
        "                match = company_found\r\n",
        "        \r\n",
        "    matched.append(match)\r\n",
        "\r\n",
        "matched_companies['registered_name'] = matched\r\n",
        "matched_companies['is_registered'] = [True]*len(matched_companies)\r\n",
        "matched_companies['is_registered'].iloc[matched_companies['registered_name'].eq('')] = False\r\n",
        "\r\n",
        "matched_companies.to_csv('./datasets/indicators/registered.csv', index=False)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "root = \"./datasets/indicators/\"\r\n",
        "\r\n",
        "# merge indicators \r\n",
        "merged_df = pd.DataFrame()\r\n",
        "first = True\r\n",
        "for file in os.listdir(root):\r\n",
        "    print(file)\r\n",
        "    metric_df = pd.read_csv(root + file)\r\n",
        "\r\n",
        "    if first:\r\n",
        "        merged_df['company_name'] = metric_df['company_name']\r\n",
        "        first = False\r\n",
        "\r\n",
        "    metric_df = metric_df.drop(metric_df[~metric_df['company_name'].isin(merged_df['company_name'])] .index)\r\n",
        "    merged_df = pd.merge(merged_df, metric_df, on='company_name', how=\"outer\")\r\n",
        "\r\n",
        "merged_df.to_csv('./datasets/overall_indicators.csv')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# open indicators\r\n",
        "with open(\"./datasets/overall_indicators.csv\", \"r\") as f:\r\n",
        "    overall_indicators = list(csv.DictReader(f))\r\n",
        "\r\n",
        "# clustering algorithm API call\r\n",
        "data = {\r\n",
        "    \"Inputs\": {\r\n",
        "        \"WebServiceInput0\": overall_indicators,\r\n",
        "    }, \"GlobalParameters\": {\r\n",
        "}\r\n",
        "}\r\n",
        " \r\n",
        "body = str.encode(json.dumps(data))\r\n",
        " \r\n",
        "url = 'http://9db7173e-c2f5-4b4c-8fb8-e73cf648193c.uksouth.azurecontainer.io/score'\r\n",
        "api_key = 'dcY6cVNnsdQQp1hLzVUPtXtWYKe5bpBY'\r\n",
        "headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key)}\r\n",
        " \r\n",
        "req = urllib.request.Request(url, body, headers)\r\n",
        "\r\n",
        "# parse API call response to obtain cluster assignments\r\n",
        "try:\r\n",
        "    response = urllib.request.urlopen(req)\r\n",
        " \r\n",
        "    result = response.read().decode(\"utf-8\") \r\n",
        "    result_json = json.loads(result)['Results']['WebServiceOutput0']\r\n",
        "    \r\n",
        "    companies_clustered = pd.DataFrame(result_json)\r\n",
        "    companies_clustered.to_csv('./companies_clustered.csv', index=False)\r\n",
        "except urllib.error.HTTPError as error:\r\n",
        "    print(\"The request failed with status code: \" + str(error.code))\r\n",
        " \r\n",
        "    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\r\n",
        "    print(error.info())\r\n",
        "    print(json.loads(error.read().decode(\"utf8\", 'ignore')))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# calculate risk score and add to overall_indicators\r\n",
        "merged_df.read_csv('./datasets/overall_indicators.csv')\r\n",
        "\r\n",
        "merged_df['risk_score'] = (1-merged_df['Assignments']) + merged_df['email_invalid'] + merged_df['phone_invalid'] + (5-merged_df['trustpilot_score'].fillna(0))\r\n",
        "merged_df.sort_values(by='risk_score', ascending=False)\r\n",
        "merged_df.to_csv('./datasets/overall_indicators.csv', index=False)"
      ],
      "outputs": [],
      "metadata": {}
    }
  ]
}